## LightGBM
LightGBM 是一个梯度 boosting 框架，使用基于学习算法的决策树。它可以说是分布式的，高效的，有以下优势：

- 更快的训练效率
- 低内存使用
- 更高的准确率
- 支持并行化学习
- 可处理大规模数据

## LightGBM 起源
GBDT模型虽然很强，但它有一个较大缺点：不能用小批量的方式训练，而要对数据进行许多次遍历。若要提升速度，就要把数据都预加载在内存中，这样数据量会受限于内存大小；即使能通过使用外存版本的决策树算法来训练数据，算法在优化，SSD也在普及，但是在频繁IO下，速度还是较慢。为了让GBDT高效使用更多数据，人们开始研究分布式GBDT，之后就有了LightGBM。

## Histogram VS Pre-sorted
#### Histogram 直方图优化算法
![image](https://img-blog.csdn.net/20180716100615151?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2h1YWNoYV9f/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70)

XGBoost中采用预排序的方法，计算过程当中是按照value的排序，逐个数据样本来计算划分收益，这样的算法能够精确的找到最佳划分值，但是代价比较大同时也没有较好的推广性。

在LightGBM中没有使用传统的预排序的思路，而是将这些精确的连续的每一个value划分到一系列离散的域中，也就是筒子里。以浮点型数据来举例，一个区间的值会被作为一个筒，然后以这些筒为精度单位的直方图来做。这样一来，数据的表达变得更加简化，减少了内存的使用，而且直方图带来了一定的正则化的效果，能够使我们做出来的模型避免过拟合且具有更好的推广性。

#### Pre-sorted
XGBoost计算分裂点时采用预排序。如果不用预排序的话，在分裂节点的时候，选中某一个特征后，需要对A按特征值大小进行排序，然后计算每个阈值的增益，这个过程需要花费很多时间。而xgb预排序的做法是，每一次残差计算好之后，对每维特征做预先排序，排好序后，对每一个维度不同阈值，计算增益，选择最佳分裂。

预排序算法的步骤是：
- 对于每个节点，遍历所有特征
- 对于每个特征，根据特征值分类样例
- 进行线性扫描，根据当前特征的基本信息增益，确定最优分割
- 选取所有特征分割结果中最好的一个

预排序算法的优点是在计算最优分裂时，各个特征的增益可以并行计算。

缺点主要是空间消耗大，预排序后需要保存特征值及排序后的索引，因此需要消耗两倍于训练数据的内存。

## Leaf-wise VS Level-wise
#### level-wise 按层生长算法
![image](https://pic3.zhimg.com/80/v2-d0a9a4137369e1b2a278206f7c6b37ba_hd.jpg)

对数据使用一次该算法，可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上很低效，因为它对同一层的叶子不加区分，带来很多不必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

#### leaf-wise 按叶子生长算法
![image](https://pic2.zhimg.com/80/v2-d274e53b478d189db3230bc5f6ad4c74_hd.jpg)

是更加高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。和level-wise相比，在分裂次数相同的情况下，Leaf-wise能降低更多误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise基础上增加了最大深度的限制，在保证高效率的同时防止过拟合。

## 特征并行和数据并行
#### 特征并行
特征并行的目的是在决策树生成过程中的每次迭代中，快速找到最优特征分裂点，其主要思想是在拥有不同特征的不同机器上分别寻找最优的分割点，然后在机器间同步最优的分割点。

LightGBM没有垂直地切分数据集，而是每个worker都有全量的训练数据，因此最优的特征分裂结果不需要传输到其他worker中，只需要将最优特征以及分裂点告诉其他worker，worker随后本地自己进行处理。处理过程如下：
- 每个worker在基于局部的特征集合找到最优分裂特征。
- workder间传输最优分裂信息，并得到全局最优分裂信息。
- 每个worker基于全局最优分裂信息，在本地进行数据分裂，生成决策树。

#### 数据并行
传统的数据并行算法：
- 水平切分数据集。
- 每个worker基于数据集构建局部特征直方图（Histogram）。
- 归并所有局部的特征直方图，得到全局直方图。
- 找到最优分裂信息，进行数据分裂。

缺点：网络传输代价比较大，如果使用point-to-point的传输算法，每个worker的传输代价为O(#machine #feature * #bin). 如果使用All Reduce并行算子，传输代价为O(2* #feature * #bin).

LightGBM算法使用Reduce Scatter并行算子归并来自不同worker的不同特征子集的直方图，然后在局部归并的直方图中找到最优局部分裂信息，最终同步找到最优的分裂信息。

## 顺序访问梯度
预排序算法中有两个频繁的操作会导致cache-miss，即缓存消失（对速度影响很大，尤其是在数据量很大的时候，顺序访问比随机访问的速度快4倍多）。

对梯度的访问：在计算增益的时候要利用梯度，对不同的特征，访问梯度的顺序不同，并且是随机的。
![image](https://2kkkkk.github.io/wiki/static/images/pca/li3.png)

对于索引表的访问：预排序算法使用了行号和叶子节点号的索引表，防止数据切分的时候对所有的特征进行切分。同访问梯度一样，所有的特征都要通过访问这个索引表来索引。

这两个操作都是随机的访问，会给系统性能带来非常大的下降。LightGBM使用的直方图算法能很好的解决这类问题。首先。对梯度的访问，因为不用对特征进行排序，同时，所有的特征都用同样的方式来访问，所以只需要对梯度访问的顺序进行重新排序，所有的特征都能连续的访问梯度。并且直方图算法不需要把数据id到叶子节点号上（不需要这个索引表，没有这个缓存消失问题）

## 支持类别特征
大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化one-hotting特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。

## 应用场景
分类问题和回归问题

## sklearn参数
#### 核心参数
1. boosting：默认值为gbdt，类型为enum，可选项有：gbdt, rf, dart, goss。
- gbdt，传统的梯度提升决策树
- rf，随机森林
- dart，Dropouts meet Multiple Additive Regression Trees
- goss，基于梯度的单侧采样
2. num_thread：线程个数
3. application：任务目标
- regression，回归
- binary，二分类
- multi-class，多分类
- cross-entropy，交叉熵
4. valid：验证集选用
5. learning_rate：学习率（梯度下降的步长）
6. num_leaves：一棵树上的叶子树
7. device：为模型学习选择设备

#### 学习控制参数
1. feature_fraction
2. bagging_fraction
3. bagging_freq
4. lambda_l1
5. lambda_l2
6. cat_smooth

#### 度量参数
- metric

## CatBoost
CatBoost（categorical boosting）是一种能够很好地处理类别型特征的梯度提升算法库。该库中的学习算法基于GPU实现，打分算法基于CPU实现。