### 一、集成学习的概念

**集成学习**：通过构建并结合多个学习器来完成学习任务。


### 二、个体学习器的概念

**个体学习器**：通常是由一个现有的学习算法从训练数据产生，多个个体学习器在集成学习中通过某种策略被结合。

如果集成中只包含同种类型的个体学习器，这样的集成是“同质”的，同质集成中的个体学习器称为“基学习器”，相应算法称为“基学习算法”。

集成也可包含不同类型的个体学习器，这样的集成是“异质”的，异质集成中的个体学习器由不同的学习算法生成。个体学习器一般不称为基学习器，常称为“组件学习器”。

### 三、boosting bagging的概念、异同点

* **boosting**：一族可将弱学习器提升为强学习器的算法。工作机制：先从初始训练集训练出一个基学习器，再根据基学习器的表现对训练样本分布进行调整，使先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布来训练下一个基学习器；如此重复，直到基学习器数目达到事先指定的值T，最终将这T个基学习器进行加权结合。
* **bagging**：bagging是并行式集成学习方法最著名的代表，工作流程：它直接基于自助采样法。给定m个样本的数据集，我们先随机取出一个样本放入采样集中，再把该样本放回初识数据集，使得下次采样时该样本仍有可能被选中，这样经过m次随机采样操作，我们得到m个样本的采样集，初始训练集中有的样本在采样集里多次出现，有的则从未出现。照这样，我们可采样出T个含m个训练样本的采样集，然后基于每个采样集训练一个基学习器，再将这些基学习器进行结合。
* **异同点**：Boosting的个体学习器间存在强依赖关系、必须串行生成的序列化方法；Bagging的个体学习器间不存在强依赖关系、可同时生成的并行化方法。

### 四、理解不同的结合策略（平均法、投票法、学习法）

**平均法**：对于数值型，最常见的结合策略是使用平均法

* 简单平均法
* 加权平均法

**投票法**：对分类任务来说，学习器将从类别标记集合中预测出一个标记，最常见的结合策略是使用投票法。

* 绝对多数投票法
* 相对多数投票法
* 加权投票法

**学习法**：当训练数据很多时，一种更强大的结合策略是使用“学习法”，即通过另一个学习器来进行结合。

### 五、随机森林的思想

随机森林（RF）是Bagging的一个扩展变体。RF在以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入了随机属性选择。传统决策树在选择划分属性时是在当前结点的属性集合（假设有d个属性）中选择一个最优属性；在RF中，对集决策树的每个结点，先从该结点的属性集合中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。这里参数k控制了随机性的引入程度。

### 六、随机森林的推广
由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用很广泛，除了可以用于分类、回归，还可以用于特征转换、异常点检测。下面列举两个代表性的算法：

- (1) extra trees: extra trees是RF的一个变种，原理几乎和RF一模一样，区别仅有：
	* 1. 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般不采用随机采样，即每个决策树采用原始训练集
	* 2. 在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较激进，它会随机选择一个特征值来划分决策树。
	* 从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，模型的方差相对于RF进一步减少，但是偏倚相对于RF进一步增大。在某些时候，extra trees的泛化能力比RF更好。

- (2) totally random trees embedding (TRTE)
	* TRTE是一种非监督学习的数据转化方法，它将低维的数据集映射到高维，从而让映射到高维的数据更好地运用于分类回归模型。在支持向量机中运用了核方法将低维的数据集映射到高维，TRTE提供了另一种方法。
	* TRTE在数据转化的过程也使用了类似于RF的方法，建立T个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在T个决策树中叶子节点的位置也定下来了。比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征xx划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0, 0,0,1,0,0, 0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。
	
### 七、随机森林的优缺点

- 优点
	* 每棵树都选择部分样本及部分特征，一定程度避免过拟合
	* 每棵树随机选择样本并随机选择特征，使得有很好的抗噪能力，性能稳定
	* 能处理很高维度的数据，且不用做特征选择
	* 适合并行运算
	* 实现比较简单
- 缺点
	* 参数较复杂
	* 模型训练和预测都比较慢

### 八、随机森林在sklearn中的参数解释

`sklearn.ensemble.RandomForestClassifier(n_estimators=100, criterion='gini',
max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None)`

- Params
	* n_estimators：森林中决策树的个数
	* criterion：采用何种方法度量分裂质量，信息熵或者基尼指数
	* max_depth：树的最大深度
	* min_samples_split：分割内部节点所需的最少样本数量
	* min_samples_leaf：叶子节点上包含的样本最小值
	* min_weight_fraction_leaf：能成为叶子节点的条件是：该节点对应的实例数和总样本数的比值，至少大于min_weight_fraction_leaf值
	* max_features：寻求最佳分割时考虑的特征数量，即特征数达到多大时进行分割 
	* max_leaf_nodes：大叶子节点数，以最好的优先方式生成树，最好的节点被定义为杂质相对较少，即纯度较高的叶子节点
	* min_impurity_decrease：一个节点会分裂，分裂之后杂质度的减少效果高于这个值
	* min_impurity_split：树增长停止的阀值。如果节点的杂质度比这个值高就会分裂，如果比这个值低，就会成为一个叶子节点
	* bootstrap：是否采用有放回式的抽样方式
	* oob_score：是否使用袋外样本来估计该模型大概的准确率
	* n_jobs：拟合和预测过程中并行运用的作业数量，如果为-1，则作业数设置为处理器的core数
	* random_state：如果是int，则是随机数生成器使用的种子；如果是RandomState实例，则是随机数生成器；如果是None，则随机数生成器是np.random使用的随机状态实例
	* verbose：控制拟合和预测时的冗长
	* warm_start：当设置为True时，重用上一个调用的解决方案来适应并向集成中添加更多的估计量，否则，只适应一个全新的foreset。
	* class_weight：如果没有给定这个值，则所有类别都应该权重为1，对于多分类问题，可按照分类结果y的可能取值的顺序给出一个list或者dict值，用来指明各类的权重。

### 九、随机森林的应用场景

- 目前随机森林在多个领域有应用：银行、医疗、股市、电子商务等等
	* 在银行领域，随机森林算法可用于发现忠诚客户，即客户经常从银行借贷并且按时还款，同样可发现欺诈客户，即没有按时还款且行为异常的人
	* 在医疗领域，随机森林算法能用于识别医药中不同成分是否以正确的方式组合。也可通过分析患者的病历识别疾病
	* 在股市方面，随机森林算法可用于识别股票的波动行为，预估损失或收益
	* 在电子商务方面，随机森林算法可根据顾客的购物经历，预测他们是否喜欢系统推荐的商品
